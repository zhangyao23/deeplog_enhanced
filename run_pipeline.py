#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced DeepLog ä¸»è¿è¡Œè„šæœ¬
"""

import sys
import os
import torch
import numpy as np
import json
import logging
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.data_preprocessing import DataPreprocessor
from src.feature_engineering import FeatureEngineer
from src.clustering import AnomalyClusterer
from src.enhanced_model import EnhancedDeepLogModel
from src.training import ModelTrainer
from src.prediction import AnomalyPredictor
from utils.data_utils import DataUtils
from utils.evaluation import ModelEvaluator

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('enhanced_deeplog.log'),
            logging.StreamHandler()
        ]
    )

def load_config(config_path: str = 'config/model_config.json') -> Dict[str, Any]:
    """åŠ è½½é…ç½®"""
    return DataUtils.load_config(config_path)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Enhanced DeepLog: åŸºäºæ”¹è¿›æ¶æ„çš„å¤šåˆ†ç±»å¼‚å¸¸æ£€æµ‹")
    print("=" * 60)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # 1. åŠ è½½é…ç½®
        logger.info("1ï¸âƒ£ åŠ è½½é…ç½®...")
        config = load_config()
        
        # 2. æ•°æ®é¢„å¤„ç†
        logger.info("2ï¸âƒ£ æ•°æ®é¢„å¤„ç†...")
        preprocessor = DataPreprocessor(config)
        
        # åŠ è½½åŸå§‹æ•°æ®ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®è·¯å¾„è°ƒæ•´ï¼‰
        data_file = '../data/hdfs_train'  # è°ƒæ•´è·¯å¾„
        if not os.path.exists(data_file):
            logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return
        
        # é¢„å¤„ç†æ•°æ®
        inputs, targets, stats = preprocessor.preprocess(data_file)
        
        # ä¿å­˜é¢„å¤„ç†ä¿¡æ¯
        preprocessor.save_preprocessing_info('data/processed/preprocessing_info.json')
        
        # 3. ç‰¹å¾å·¥ç¨‹
        logger.info("3ï¸âƒ£ ç‰¹å¾å·¥ç¨‹...")
        feature_engineer = FeatureEngineer(config)
        
        # æå–ç‰¹å¾ï¼ˆè¿™é‡Œéœ€è¦é¢„æµ‹è¯¯å·®ï¼Œæš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
        prediction_errors = np.random.random(len(inputs))  # æ¨¡æ‹Ÿé¢„æµ‹è¯¯å·®
        features = feature_engineer.extract_all_features(inputs, prediction_errors)
        
        # ä¿å­˜ç‰¹å¾
        DataUtils.save_numpy_data(features, 'data/features/extracted_features.npy')
        
        # 4. èšç±»åˆ†æ
        logger.info("4ï¸âƒ£ èšç±»åˆ†æ...")
        clusterer = AnomalyClusterer(config)
        cluster_labels = clusterer.fit(features)
        
        # è·å–èšç±»ä¿¡æ¯
        cluster_info = clusterer.get_cluster_info()
        logger.info(f"èšç±»ç»“æœ: {cluster_info}")
        
        # 5. åˆ›å»ºæ¨¡å‹
        logger.info("5ï¸âƒ£ åˆ›å»ºæ¨¡å‹...")
        num_event_types = config['data_config']['num_event_types']
        num_anomaly_types = config['clustering_config']['n_clusters']
        
        model = EnhancedDeepLogModel(config, num_event_types, num_anomaly_types)
        model_info = model.get_model_info()
        logger.info(f"æ¨¡å‹ä¿¡æ¯: {model_info}")
        
        # 6. å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("6ï¸âƒ£ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        trainer = ModelTrainer(config, model)
        
        # ä½¿ç”¨èšç±»æ ‡ç­¾ä½œä¸ºå¼‚å¸¸åˆ†ç±»ç›®æ ‡
        train_loader, val_loader = trainer.prepare_data(
            inputs, targets, cluster_labels
        )
        
        # 7. è®­ç»ƒæ¨¡å‹
        logger.info("7ï¸âƒ£ è®­ç»ƒæ¨¡å‹...")
        training_results = trainer.train(train_loader, val_loader, device)
        
        # ä¿å­˜è®­ç»ƒå†å²
        trainer.save_training_history('results/training_history.json')
        
        # 8. æ¨¡å‹è¯„ä¼°
        logger.info("8ï¸âƒ£ æ¨¡å‹è¯„ä¼°...")
        evaluator = ModelEvaluator()
        
        # åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
        predictor = AnomalyPredictor(model, config)
        predictions = predictor.predict(inputs, device)
        
        # è¯„ä¼°åˆ†ç±»æ€§èƒ½
        classification_metrics = evaluator.evaluate_classification(
            cluster_labels, predictions['anomaly_predictions']
        )
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        evaluator.print_evaluation_summary(classification_metrics)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        evaluation_results = {
            'classification_metrics': classification_metrics,
            'cluster_info': cluster_info,
            'model_info': model_info,
            'training_results': training_results
        }
        evaluator.save_evaluation_results(evaluation_results, 'results/evaluation_results.json')
        
        # 9. ä¿å­˜æ¨¡å‹
        logger.info("9ï¸âƒ£ ä¿å­˜æ¨¡å‹...")
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config,
            'model_info': model_info,
            'cluster_info': cluster_info
        }, 'models/enhanced_deeplog_model.pth')
        
        logger.info("âœ… Enhanced DeepLog è®­ç»ƒå®Œæˆï¼")
        logger.info("ğŸ“Š ç»“æœæ–‡ä»¶:")
        logger.info("  - æ¨¡å‹: models/enhanced_deeplog_model.pth")
        logger.info("  - è®­ç»ƒå†å²: results/training_history.json")
        logger.info("  - è¯„ä¼°ç»“æœ: results/evaluation_results.json")
        logger.info("  - é¢„å¤„ç†ä¿¡æ¯: data/processed/preprocessing_info.json")
        logger.info("  - ç‰¹å¾: data/features/extracted_features.npy")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 