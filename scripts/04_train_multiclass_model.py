#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è„šæœ¬ 04: è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹

æ­¤è„šæœ¬æ‰§è¡Œ Enhanced DeepLog æ¨¡å‹çš„ç¬¬å››ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªé˜¶æ®µï¼š
1. åŠ è½½é˜¶æ®µäºŒç”Ÿæˆçš„ç‰¹å¾çŸ©é˜µå’Œé˜¶æ®µä¸‰ç”Ÿæˆçš„æœ€ç»ˆèšç±»æ ‡ç­¾ã€‚
2. åŠ è½½åŸå§‹çš„ã€ç»è¿‡å¡«å……çš„åºåˆ—æ•°æ®ï¼Œå› ä¸ºå¢å¼ºæ¨¡å‹éœ€è¦å®ƒä½œä¸ºè¾“å…¥ã€‚
3. å°†æ•°æ®ï¼ˆåºåˆ—ã€ç‰¹å¾ã€æ ‡ç­¾ï¼‰åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
4. å®šä¹‰å¹¶è®­ç»ƒ `EnhancedDeepLogModel`ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼ŒåŒæ—¶è¿›è¡Œåºåˆ—é¢„æµ‹å’Œå¼‚å¸¸åˆ†ç±»ã€‚
5. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦ç­‰ç­–ç•¥æ¥ä¼˜åŒ–æ€§èƒ½ã€‚
6. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¿å­˜åˆ†ç±»æŠ¥å‘Šå’Œæ··æ·†çŸ©é˜µã€‚
7. ä¿å­˜è®­ç»ƒå¥½çš„å¤šåˆ†ç±»æ¨¡å‹ä»¥ä¾›å°†æ¥é¢„æµ‹ä½¿ç”¨ã€‚
"""

import os
import sys
import logging
import torch
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from enhanced_deeplog.src.enhanced_model import EnhancedDeepLogModel
from enhanced_deeplog.src.training import ModelTrainer
from enhanced_deeplog.utils.data_utils import DataUtils
from enhanced_deeplog.utils.evaluation import ModelEvaluator
from enhanced_deeplog.src.data_preprocessing import DataPreprocessor # éœ€è¦ç”¨å®ƒæ¥åŠ è½½åŸå§‹åºåˆ—

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå¤šåˆ†ç±»æ¨¡å‹çš„è®­ç»ƒã€è¯„ä¼°å’Œä¿å­˜æµç¨‹ã€‚
    """
    print("ğŸš€ Enhanced DeepLog - é˜¶æ®µå››ï¼šè®­ç»ƒå¤šåˆ†ç±»æ³¨æ„åŠ›æ¨¡å‹ ğŸš€")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    config_path = 'enhanced_deeplog/config/model_config.json'
    if not os.path.exists(config_path):
        logging.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        return
    config = DataUtils.load_config(config_path)
    
    # 2. åŠ è½½æ•°æ®
    # åŠ è½½ç‰¹å¾å’Œæœ€ç»ˆæ ‡ç­¾
    features_path = os.path.join(config['data_config']['processed_dir'], 'features.npy')
    labels_path = os.path.join(config['data_config']['processed_dir'], 'final_labels.npy')
    
    if not os.path.exists(features_path) or not os.path.exists(labels_path):
        logging.error("ç‰¹å¾æˆ–æœ€ç»ˆæ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œ 02_extract_features.py å’Œ 03_anomaly_clustering.pyã€‚")
        return

    logging.info("åŠ è½½ç‰¹å¾å’Œèšç±»æ ‡ç­¾...")
    features = DataUtils.load_numpy_data(features_path)
    labels = DataUtils.load_numpy_data(labels_path)

    # åŠ è½½åŸå§‹åºåˆ—æ•°æ®å¹¶è¿›è¡Œå¤„ç†
    logging.info("åŠ è½½å¹¶å¤„ç†åŸå§‹åºåˆ—æ•°æ®ä»¥ç”¨äºæ¨¡å‹è¾“å…¥...")
    raw_data_path = os.path.join(config['data_config']['raw_data_dir'], config['data_config']['data_file'])
    raw_sequences = DataUtils.load_sequences(raw_data_path)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹ä»¥è·å– event_mapping
    base_model_path = config['model_config']['base_model_path']
    if not os.path.exists(base_model_path):
        logging.error(f"åŸºç¡€æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {base_model_path}")
        return
    checkpoint = torch.load(base_model_path)
    event_mapping = checkpoint['config']['event_mapping']
    
    # ä½¿ç”¨ä¸ç‰¹å¾æå–ç›¸åŒçš„é€»è¾‘æ¥ç”Ÿæˆçª—å£
    preprocessor = DataPreprocessor(config)
    preprocessor.event_mapping = event_mapping
    
    all_windows = []
    for seq in raw_sequences:
        windows, _ = preprocessor.split_and_pad_sequence(seq, window_size=config['data_config']['window_size'])
        if windows.shape[0] > 0:
            all_windows.append(windows)
    
    sequences_padded = np.vstack(all_windows)
    
    # ç¡®ä¿æ‰€æœ‰æ•°æ®æºçš„æ ·æœ¬æ•°é‡ä¸€è‡´
    if not (len(sequences_padded) == len(features) == len(labels)):
        logging.error(f"æ•°æ®ä¸å¯¹é½ï¼åºåˆ—: {len(sequences_padded)}, ç‰¹å¾: {len(features)}, æ ‡ç­¾: {len(labels)}")
        return

    # 3. æ•°æ®é›†åˆ†å‰²
    logging.info(f"å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†...")
    
    # é¦–å…ˆï¼Œåˆ†å‰²å‡ºæµ‹è¯•é›† (e.g., 20%)
    indices = np.arange(len(sequences_padded))
    X_train_val_idx, X_test_idx = train_test_split(
        indices, 
        test_size=0.2, 
        random_state=42, 
        stratify=labels
    )
    
    # ç„¶åï¼Œä»å‰©ä½™æ•°æ®ä¸­åˆ†å‰²å‡ºè®­ç»ƒé›†å’ŒéªŒè¯é›† (e.g., 25% of the rest, which is 20% of original)
    y_train_val = labels[X_train_val_idx]
    X_train_idx, X_val_idx = train_test_split(
        X_train_val_idx,
        test_size=0.25, 
        random_state=42,
        stratify=y_train_val
    )

    # åˆ›å»ºæ•°æ®é›†
    X_train_seq, X_val_seq, X_test_seq = sequences_padded[X_train_idx], sequences_padded[X_val_idx], sequences_padded[X_test_idx]
    X_train_feat, X_val_feat, X_test_feat = features[X_train_idx], features[X_val_idx], features[X_test_idx]
    y_train, y_val, y_test = labels[X_train_idx], labels[X_val_idx], labels[X_test_idx]
    
    logging.info(f"è®­ç»ƒé›†å¤§å°: {len(X_train_seq)}")
    logging.info(f"éªŒè¯é›†å¤§å°: {len(X_val_seq)}")
    logging.info(f"æµ‹è¯•é›†å¤§å°: {len(X_test_seq)}")

    # 4. åˆå§‹åŒ–æ¨¡å‹
    num_keys = len(event_mapping)
    num_classes = len(np.unique(labels))

    model = EnhancedDeepLogModel(
        config=config,
        num_event_types=num_keys,
        num_anomaly_types=num_classes,
        feature_dim=features.shape[1]
    )
    
    # 5. è®­ç»ƒæ¨¡å‹
    trainer = ModelTrainer(config, model)
    
    model_save_path = 'enhanced_deeplog/models/multiclass_model.pth'
    
    logging.info("ğŸš€ å¼€å§‹è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹...")
    trainer.train(
        (X_train_seq, X_train_feat, y_train),
        (X_val_seq, X_val_feat, y_val),
        model_save_path
    )
    logging.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
    
    # 6. è¯„ä¼°æ¨¡å‹
    logging.info("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    # trainerå†…éƒ¨å·²ç»åŠ è½½äº†æœ€ä½³æ¨¡å‹
    evaluator = ModelEvaluator(config)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_dataset = TensorDataset(
        torch.from_numpy(X_test_seq).float(), 
        torch.from_numpy(X_test_feat).float(),
        torch.from_numpy(y_test).long()
    )
    test_loader = DataLoader(test_dataset, batch_size=config['training_config']['batch_size'], shuffle=False)

    # è·å–é¢„æµ‹ç»“æœ
    y_true, y_pred_indices, y_pred_probs = trainer.predict(test_loader)
    
    # ç”Ÿæˆå’Œæ‰“å°åˆ†ç±»æŠ¥å‘Š
    class_names = [f"Class_{i}" for i in range(num_classes)]
    report_str = evaluator.generate_classification_report(y_true, y_pred_indices, class_names)
    print("\nğŸ“Š åˆ†ç±»æ€§èƒ½æŠ¥å‘Š (æµ‹è¯•é›†) ğŸ“Š")
    print("="*60)
    print(report_str)
    print("="*60)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = 'enhanced_deeplog/diagnostics/classification_report.txt'
    DataUtils.ensure_directory(os.path.dirname(report_path))
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_str)
    logging.info(f"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    # ç»˜åˆ¶å’Œä¿å­˜æ··æ·†çŸ©é˜µ
    cm_path = 'enhanced_deeplog/diagnostics/confusion_matrix.png'
    evaluator.plot_confusion_matrix(y_true, y_pred_indices, class_names=class_names, save_path=cm_path)
    logging.info(f"æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜åˆ°: {cm_path}")
    
    logging.info("ğŸ‰ é˜¶æ®µå››å®Œæˆï¼æ•´ä¸ªæµç¨‹ç»“æŸã€‚ğŸ‰")

if __name__ == "__main__":
    main() 