#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è„šæœ¬ 05: åˆ†æèšç±»ç»“æœ

æ­¤è„šæœ¬ç”¨äºæ·±å…¥ç†è§£åœ¨é˜¶æ®µä¸‰ä¸­é€šè¿‡èšç±»ç”Ÿæˆçš„ä¸åŒç±»åˆ«ï¼ˆClass 0-3ï¼‰ä»£è¡¨äº†ä½•ç§æ—¥å¿—è¡Œä¸ºã€‚
å®ƒä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1.  åŠ è½½é…ç½®æ–‡ä»¶ã€æœ€ç»ˆçš„èšç±»æ ‡ç­¾ä»¥åŠåŸå§‹çš„ã€ç»è¿‡å¤„ç†çš„åºåˆ—æ•°æ®ã€‚
2.  åŠ è½½åŸºç¡€æ¨¡å‹ä»¥è·å–äº‹ä»¶IDåˆ°æ¨¡æ¿çš„æ˜ å°„å…³ç³»ã€‚
3.  ä¸ºæ¯ä¸ªç±»åˆ«ï¼ˆç‰¹åˆ«æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„ç±»åˆ«ï¼‰éšæœºæŠ½å–å‡ ä¸ªæ—¥å¿—çª—å£æ ·æœ¬ã€‚
4.  å°†è¿™äº›æ•°å­—åºåˆ—è§£ç å›äººç±»å¯è¯»çš„æ—¥å¿—æ¨¡æ¿ã€‚
5.  æ‰“å°ç»“æœï¼Œä»¥ä¾¿åˆ†ææ¯ä¸ªç±»åˆ«æ‰€ä»£è¡¨çš„å…·ä½“æ—¥å¿—æ¨¡å¼ã€‚
"""

import os
import sys
import logging
import torch
import numpy as np
import json

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from enhanced_deeplog.utils.data_utils import DataUtils
from enhanced_deeplog.src.data_preprocessing import DataPreprocessor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def analyze_clusters(config_path: str, num_samples_per_class: int = 5):
    """
    ä¸»åˆ†æå‡½æ•°ï¼ŒåŠ è½½æ•°æ®å¹¶æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ—¥å¿—æ ·æœ¬ã€‚

    Args:
        config_path (str): é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚
        num_samples_per_class (int): æ¯ä¸ªç±»åˆ«è¦æŠ½å–çš„æ ·æœ¬æ•°é‡ã€‚
    """
    print("ğŸš€ Enhanced DeepLog - èšç±»ç»“æœåˆ†æ ğŸš€")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    if not os.path.exists(config_path):
        logging.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        return
    config = DataUtils.load_config(config_path)

    # 2. åŠ è½½æ•°æ®
    # åŠ è½½æœ€ç»ˆæ ‡ç­¾
    labels_path = os.path.join(config['data_config']['processed_dir'], 'final_labels.npy')
    if not os.path.exists(labels_path):
        logging.error("æœ€ç»ˆæ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œ 03_anomaly_clustering.pyã€‚")
        return
    
    logging.info("åŠ è½½èšç±»æ ‡ç­¾...")
    labels = DataUtils.load_numpy_data(labels_path)

    # åŠ è½½åŸå§‹åºåˆ—æ•°æ®å¹¶è¿›è¡Œå¤„ç†ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    logging.info("åŠ è½½å¹¶å¤„ç†åŸå§‹åºåˆ—æ•°æ®...")
    raw_data_path = os.path.join(config['data_config']['raw_data_dir'], config['data_config']['data_file'])
    raw_sequences = DataUtils.load_sequences(raw_data_path)

    # åŠ è½½åŸºç¡€æ¨¡å‹ä»¥è·å– event_mapping
    base_model_path = config['model_config']['base_model_path']
    if not os.path.exists(base_model_path):
        logging.error(f"åŸºç¡€æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {base_model_path}")
        return
    checkpoint = torch.load(base_model_path)
    event_mapping = checkpoint['config']['event_mapping']
    
    # åˆ›å»ºåå‘æ˜ å°„ (ID -> Template)
    id_to_event_mapping = {v: k for k, v in event_mapping.items()}
    # æ·»åŠ å¯¹ <PAD> å’Œ <UNK> çš„å¤„ç†
    id_to_event_mapping[0] = "<PAD>"
    id_to_event_mapping.setdefault(1, "<UNK>") # å‡å®š 1 æ˜¯ <UNK>

    # ä½¿ç”¨ä¸ç‰¹å¾æå–ç›¸åŒçš„é€»è¾‘æ¥ç”Ÿæˆçª—å£
    preprocessor = DataPreprocessor(config)
    preprocessor.event_mapping = event_mapping
    
    all_windows = []
    for seq in raw_sequences:
        windows, _ = preprocessor.split_and_pad_sequence(seq, window_size=config['data_config']['window_size'])
        if windows.shape[0] > 0:
            all_windows.append(windows)
    
    sequences_padded = np.vstack(all_windows)

    # ç¡®ä¿æ•°æ®å¯¹é½
    if len(sequences_padded) != len(labels):
        logging.error(f"æ•°æ®ä¸å¯¹é½ï¼åºåˆ—: {len(sequences_padded)}, æ ‡ç­¾: {len(labels)}")
        return

    # 3. åˆ†ææ¯ä¸ªç±»åˆ«
    unique_labels = sorted(np.unique(labels))
    logging.info(f"å‘ç°ç±»åˆ«: {unique_labels}")

    for label in unique_labels:
        print("\n" + "="*60)
        print(f"ğŸ”¬ åˆ†æ Class_{label}...")
        print("="*60)

        # æ‰¾åˆ°å±äºå½“å‰ç±»åˆ«çš„æ‰€æœ‰åºåˆ—çš„ç´¢å¼•
        indices = np.where(labels == label)[0]
        
        # éšæœºé€‰æ‹©å‡ ä¸ªæ ·æœ¬
        num_to_sample = min(num_samples_per_class, len(indices))
        if num_to_sample == 0:
            print(f"Class_{label} ä¸­æ²¡æœ‰æ ·æœ¬ã€‚")
            continue
        
        sample_indices = np.random.choice(indices, num_to_sample, replace=False)
        
        print(f"ä» {len(indices)} ä¸ªæ€»æ ·æœ¬ä¸­éšæœºæŠ½å– {num_to_sample} ä¸ªè¿›è¡Œå±•ç¤º:\n")

        for i, idx in enumerate(sample_indices):
            sequence = sequences_padded[idx]
            decoded_sequence = [id_to_event_mapping.get(event_id, f"<ID_{event_id}?>") for event_id in sequence if event_id != 0] # è¿‡æ»¤æ‰ PAD
            
            print(f"--- æ ·æœ¬ {i+1} (åŸå§‹ç´¢å¼•: {idx}) ---")
            for step, template in enumerate(decoded_sequence):
                print(f"  Step {step}: {template}")
            print("-"*(len(str(i+1))+13))


if __name__ == "__main__":
    analyze_clusters(config_path='enhanced_deeplog/config/model_config.json')
