#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µä¸€ï¼šè®­ç»ƒâ€œæ­£å¸¸æ¨¡å¼ä¸“å®¶â€æ¨¡å‹
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import json
import logging
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.data_preprocessing import DataPreprocessor
from src.base_model import SimpleLSTM
from utils.data_utils import DataUtils

# --- æ—¥å¿—å’Œè®¾å¤‡é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_and_prepare_data(config, data_path, label_path):
    """åŠ è½½æ‰€æœ‰æ•°æ®ä»¥æ„å»ºå®Œæ•´çš„äº‹ä»¶æ˜ å°„ï¼Œç„¶åç­›é€‰å‡ºæ­£å¸¸æ•°æ®è¿›è¡Œå¤„ç†ã€‚"""
    logging.info("åŠ è½½æ‰€æœ‰æ•°æ®ä»¥æ„å»ºå…¨å±€äº‹ä»¶æ˜ å°„...")
    
    # åŠ è½½æ‰€æœ‰åºåˆ—æ¥æ„å»ºæ˜ å°„
    with open(data_path, 'r') as f:
        all_sequences = [list(map(int, line.strip().split())) for line in f]
    
    # ä½¿ç”¨DataPreprocessoræ¥åˆ›å»ºå…¨å±€æ˜ å°„
    preprocessor = DataPreprocessor(config)
    preprocessor.create_event_mapping(all_sequences)
    num_total_events = len(preprocessor.event_mapping)
    logging.info(f"å…¨å±€äº‹ä»¶æ˜ å°„åˆ›å»ºå®Œæˆï¼Œå…± {num_total_events} ä¸ªå”¯ä¸€äº‹ä»¶ã€‚")
    
    # æ›´æ–°é…ç½®
    config['data_config']['num_event_types'] = num_total_events

    # ç°åœ¨ï¼ŒåŠ è½½å¹¶ç­›é€‰æ­£å¸¸æ•°æ®
    logging.info("åŠ è½½å¹¶ç­›é€‰æ­£å¸¸æ•°æ®ç”¨äºè®­ç»ƒ...")
    with open(label_path, 'r') as f:
        all_labels = [int(line.strip()) for line in f]

    normal_sequences_raw = [seq for seq, label in zip(all_sequences, all_labels) if label == 0]
    
    # åº”ç”¨å·²åˆ›å»ºçš„å…¨å±€æ˜ å°„
    mapped_sequences = preprocessor.apply_event_mapping(normal_sequences_raw)
    
    # æ»‘çª—
    window_size = config['data_config']['window_size']
    inputs, targets = preprocessor.split_sequences(mapped_sequences, window_size)

    # è¡¥é½
    padded_inputs = preprocessor.pad_sequences(inputs, pad_to_length=window_size)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_inputs, val_inputs, train_targets, val_targets = train_test_split(
        padded_inputs, np.array(targets), test_size=0.2, random_state=42
    )
    
    logging.info(f"æ•°æ®åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_inputs)}æ¡, éªŒè¯é›† {len(val_inputs)}æ¡")
    
    return train_inputs, val_inputs, train_targets, val_targets, config, preprocessor

def train_base_model(config, train_inputs, val_inputs, train_targets, val_targets, model_save_path, preprocessor):
    """ä½¿ç”¨æ—©åœæœºåˆ¶è®­ç»ƒåŸºç¡€æ¨¡å‹"""
    logging.info("å¼€å§‹ä½¿ç”¨æ—©åœæœºåˆ¶è®­ç»ƒåŸºç¡€æ¨¡å‹...")
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(torch.FloatTensor(train_inputs), torch.LongTensor(train_targets))
    train_loader = DataLoader(train_dataset, batch_size=config['training_config']['batch_size'], shuffle=True)
    
    val_dataset = TensorDataset(torch.FloatTensor(val_inputs), torch.LongTensor(val_targets))
    val_loader = DataLoader(val_dataset, batch_size=config['training_config']['batch_size'], shuffle=False)
    
    # åˆ›å»ºç®€åŒ–çš„åŸºç¡€æ¨¡å‹
    num_event_types = config['data_config']['num_event_types']
    model = SimpleLSTM(
        input_size=1,
        hidden_size=64,
        num_layers=2,
        num_keys=num_event_types
    )
    model.to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    
    # æ—©åœæœºåˆ¶å‚æ•°
    patience = 5
    best_val_loss = float('inf')
    patience_counter = 0
    
    num_epochs = 80
    
    for epoch in range(num_epochs):
        # --- è®­ç»ƒ ---
        model.train()
        epoch_train_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [è®­ç»ƒ]")
        for batch_inputs, batch_targets in pbar:
            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)
            optimizer.zero_grad()
            seq_pred, _ = model(batch_inputs)
            loss = criterion(seq_pred, batch_targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config['training_config']['gradient_clip'])
            optimizer.step()
            epoch_train_loss += loss.item()
            pbar.set_postfix({'train_loss': loss.item()})
        avg_train_loss = epoch_train_loss / len(train_loader)

        # --- éªŒè¯ ---
        model.eval()
        epoch_val_loss = 0
        with torch.no_grad():
            for batch_inputs, batch_targets in val_loader:
                batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)
                seq_pred, _ = model(batch_inputs)
                loss = criterion(seq_pred, batch_targets)
                epoch_val_loss += loss.item()
        avg_val_loss = epoch_val_loss / len(val_loader)
        
        logging.info(f"Epoch {epoch + 1}/{num_epochs}, è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, éªŒè¯æŸå¤±: {avg_val_loss:.4f}")

        # --- æ—©åœåˆ¤æ–­ ---
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æ—¶ä¸€å¹¶ä¿å­˜event_mapping
            config['event_mapping'] = preprocessor.event_mapping
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': config,
                'model_class': 'SimpleLSTM'
            }, model_save_path)
            logging.info(f"ğŸ‰ éªŒè¯æŸå¤±é™ä½ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {model_save_path}")
        else:
            patience_counter += 1
            logging.info(f"éªŒè¯æŸå¤±æœªé™ä½ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{patience}")
        
        if patience_counter >= patience:
            logging.info("ğŸš« è§¦å‘æ—©åœæœºåˆ¶ï¼Œè®­ç»ƒç»“æŸã€‚")
            break
            
    return model

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Enhanced DeepLog - é˜¶æ®µä¸€ï¼šè®­ç»ƒâ€œæ­£å¸¸æ¨¡å¼ä¸“å®¶â€æ¨¡å‹ ğŸš€")
    print("="*60)
    
    # åŠ è½½é…ç½®
    config_path = 'config/model_config.json'
    if not os.path.exists(os.path.join('enhanced_deeplog', config_path)):
        logging.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        return
    config = DataUtils.load_config(os.path.join('enhanced_deeplog', config_path))

    # æ•°æ®è·¯å¾„
    data_path = 'enhanced_deeplog/data/raw/synthetic_hdfs_train.txt'
    label_path = 'enhanced_deeplog/data/raw/synthetic_hdfs_train_labels.txt'

    if not os.path.exists(data_path):
        logging.error("è¯·å…ˆè¿è¡Œ 00_generate_synthetic_data.py ç”Ÿæˆæ•°æ®ã€‚")
        return
        
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
    train_inputs, val_inputs, train_targets, val_targets, updated_config, preprocessor = load_and_prepare_data(
        config, 
        data_path, 
        label_path
    )
    
    # 2. è®­ç»ƒåŸºç¡€æ¨¡å‹
    model_dir = 'enhanced_deeplog/models'
    DataUtils.ensure_directory(model_dir)
    model_save_path = os.path.join(model_dir, 'base_model.pth')
    
    base_model = train_base_model(updated_config, train_inputs, val_inputs, train_targets, val_targets, model_save_path, preprocessor)
    
    logging.info(f"âœ… è®­ç»ƒå®Œæˆã€‚æœ€ä½³æ¨¡å‹å·²ä¿å­˜åœ¨: {model_save_path}")

if __name__ == "__main__":
    main() 