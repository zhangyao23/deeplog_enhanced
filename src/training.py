#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒæ¨¡å—
è´Ÿè´£æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import Dict, Any, Tuple, List
import logging
import json
from tqdm import tqdm
import os
import torch.nn.functional as F

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any], model: nn.Module):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: é…ç½®å­—å…¸
            model: æ¨¡å‹å®ä¾‹
        """
        self.config = config
        self.training_config = config['training_config']
        self.model = model
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = self.training_config['batch_size']
        self.learning_rate = self.training_config['learning_rate']
        self.num_epochs = self.training_config['num_epochs']
        self.sequence_loss_weight = self.training_config['sequence_loss_weight']
        self.anomaly_loss_weight = self.training_config['anomaly_loss_weight']
        self.weight_decay = self.training_config['weight_decay']
        self.gradient_clip = self.training_config['gradient_clip']
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'sequence_loss': [],
            'anomaly_loss': [],
            'total_loss': [],
            'sequence_accuracy': [],
            'anomaly_accuracy': []
        }
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def train(self, train_data: Tuple, val_data: Tuple, model_save_path: str) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_data: åŒ…å«è®­ç»ƒåºåˆ—ã€ç‰¹å¾å’Œæ ‡ç­¾çš„å…ƒç»„ (X_train_seq, X_train_feat, y_train)
            val_data: åŒ…å«éªŒè¯åºåˆ—ã€ç‰¹å¾å’Œæ ‡ç­¾çš„å…ƒç»„ (X_val_seq, X_val_feat, y_val)
            model_save_path: æœ€ä½³æ¨¡å‹çš„ä¿å­˜è·¯å¾„
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        X_train_seq, X_train_feat, y_train_anomaly = train_data
        X_val_seq, X_val_feat, y_val_anomaly = val_data

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)

        # è·å–ç”¨äºåºåˆ—é¢„æµ‹çš„ç›®æ ‡ (ä¸‹ä¸€ä¸ªäº‹ä»¶)
        # è¿™é‡Œçš„ç›®æ ‡æ˜¯è¾“å…¥çª—å£ä¸­çš„æœ€åä¸€ä¸ªäº‹ä»¶
        y_train_seq = np.array([seq[-1] for seq in X_train_seq])
        y_val_seq = np.array([seq[-1] for seq in X_val_seq])

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(
            torch.from_numpy(X_train_seq).float(),
            torch.from_numpy(X_train_feat).float(),
            torch.from_numpy(y_train_seq).long(),
            torch.from_numpy(y_train_anomaly).long()
        )
        val_dataset = TensorDataset(
            torch.from_numpy(X_val_seq).float(),
            torch.from_numpy(X_val_feat).float(),
            torch.from_numpy(y_val_seq).long(),
            torch.from_numpy(y_val_anomaly).long()
        )
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)

        self.logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        self.logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)
        
        # æŸå¤±å‡½æ•°
        sequence_criterion = nn.CrossEntropyLoss()
        anomaly_criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.learning_rate, 
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.7, patience=3
        )
        
        # æ—©åœ
        best_val_loss = float('inf')
        patience_counter = 0
        early_stopping_patience = self.training_config['early_stopping_patience']
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_metrics = self._train_epoch(
                train_loader, optimizer, sequence_criterion, anomaly_criterion, device
            )
            
            # éªŒè¯é˜¶æ®µ
            val_metrics = self._validate_epoch(
                val_loader, sequence_criterion, anomaly_criterion, device
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_metrics['total_loss'])
            
            # è®°å½•å†å²
            for key in self.train_history:
                if key in train_metrics:
                    self.train_history[key].append(train_metrics[key])
            
            # æ—©åœæ£€æŸ¥
            if val_metrics['total_loss'] < best_val_loss:
                best_val_loss = val_metrics['total_loss']
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({'model_state_dict': self.model.state_dict(), 'config': self.config}, model_save_path)
                self.logger.info(f"ğŸ‰ Val loss improved to {best_val_loss:.4f}. Saving best model.")
            else:
                patience_counter += 1
                self.logger.info(f"Val loss did not improve. Patience: {patience_counter}/{early_stopping_patience}")
            
            # æ‰“å°è¿›åº¦
            self.logger.info(
                f"Epoch [{epoch+1}/{self.num_epochs}] | "
                f"Train Loss: {train_metrics['total_loss']:.4f} | "
                f"Val Loss: {val_metrics['total_loss']:.4f} | "
                f"Train Anomaly Acc: {train_metrics['anomaly_accuracy']:.4f}"
            )

            # æ—©åœ
            if patience_counter >= early_stopping_patience:
                self.logger.info(f"ğŸš« Early stopping triggered at epoch {epoch+1}")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.logger.info(f"Loading best model from {model_save_path}")
        self.model.load_state_dict(torch.load(model_save_path)['model_state_dict'])
        
        return {
            'train_history': self.train_history,
            'best_val_loss': best_val_loss,
            'final_epoch': epoch + 1
        }
    
    def _train_epoch(self, train_loader: DataLoader, optimizer: optim.Optimizer,
                    sequence_criterion: nn.Module, anomaly_criterion: nn.Module,
                    device: torch.device) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_sequence_loss = 0
        total_anomaly_loss = 0
        total_loss = 0
        sequence_correct = 0
        anomaly_correct = 0
        total_samples = 0
        
        for batch_seq, batch_feat, batch_seq_targets, batch_anomaly_targets in tqdm(train_loader, desc=f"Epoch {self.train_history.get('total_loss', [0]).__len__()} Training"):
            batch_seq = batch_seq.to(device)
            batch_feat = batch_feat.to(device)
            batch_seq_targets = batch_seq_targets.to(device)
            batch_anomaly_targets = batch_anomaly_targets.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            sequence_pred, anomaly_pred = self.model(batch_seq, batch_feat)
            
            # è®¡ç®—æŸå¤±
            sequence_loss = sequence_criterion(sequence_pred, batch_seq_targets)
            anomaly_loss = anomaly_criterion(anomaly_pred, batch_anomaly_targets)
            total_batch_loss = (self.sequence_loss_weight * sequence_loss + 
                              self.anomaly_loss_weight * anomaly_loss)
            
            # åå‘ä¼ æ’­
            total_batch_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
            
            optimizer.step()
            
            # ç»Ÿè®¡
            total_sequence_loss += sequence_loss.item()
            total_anomaly_loss += anomaly_loss.item()
            total_loss += total_batch_loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            sequence_correct += (torch.argmax(sequence_pred, dim=1) == batch_seq_targets).sum().item()
            anomaly_correct += (torch.argmax(anomaly_pred, dim=1) == batch_anomaly_targets).sum().item()
            total_samples += batch_seq.size(0)
        
        return {
            'sequence_loss': total_sequence_loss / len(train_loader),
            'anomaly_loss': total_anomaly_loss / len(train_loader),
            'total_loss': total_loss / len(train_loader),
            'sequence_accuracy': sequence_correct / total_samples,
            'anomaly_accuracy': anomaly_correct / total_samples
        }
    
    def _validate_epoch(self, val_loader: DataLoader, sequence_criterion: nn.Module,
                       anomaly_criterion: nn.Module, device: torch.device) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_sequence_loss = 0
        total_anomaly_loss = 0
        total_loss = 0
        
        with torch.no_grad():
            for batch_seq, batch_feat, batch_seq_targets, batch_anomaly_targets in tqdm(val_loader, desc="Validating"):
                batch_seq = batch_seq.to(device)
                batch_feat = batch_feat.to(device)
                batch_seq_targets = batch_seq_targets.to(device)
                batch_anomaly_targets = batch_anomaly_targets.to(device)
                
                # å‰å‘ä¼ æ’­
                sequence_pred, anomaly_pred = self.model(batch_seq, batch_feat)
                
                # è®¡ç®—æŸå¤±
                sequence_loss = sequence_criterion(sequence_pred, batch_seq_targets)
                anomaly_loss = anomaly_criterion(anomaly_pred, batch_anomaly_targets)
                total_batch_loss = (self.sequence_loss_weight * sequence_loss + 
                                  self.anomaly_loss_weight * anomaly_loss)
                
                # ç»Ÿè®¡
                total_sequence_loss += sequence_loss.item()
                total_anomaly_loss += anomaly_loss.item()
                total_loss += total_batch_loss.item()
        
        return {
            'sequence_loss': total_sequence_loss / len(val_loader),
            'anomaly_loss': total_anomaly_loss / len(val_loader),
            'total_loss': total_loss / len(val_loader)
        }
    
    def predict(self, test_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ã€‚

        Args:
            test_loader (DataLoader): æµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: (çœŸå®æ ‡ç­¾, é¢„æµ‹ç´¢å¼•, é¢„æµ‹æ¦‚ç‡)
        """
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)
        self.model.eval()

        all_true_labels = []
        all_pred_indices = []
        all_pred_probs = []

        with torch.no_grad():
            for batch_seq, batch_feat, batch_anomaly_targets in tqdm(test_loader, desc="Predicting"):
                batch_seq = batch_seq.to(device)
                batch_feat = batch_feat.to(device)
                
                # å‰å‘ä¼ æ’­
                _, anomaly_pred = self.model(batch_seq, batch_feat)
                
                # è·å–é¢„æµ‹ç»“æœ
                pred_probs = F.softmax(anomaly_pred, dim=1)
                pred_indices = torch.argmax(pred_probs, dim=1)

                all_true_labels.extend(batch_anomaly_targets.cpu().numpy())
                all_pred_indices.extend(pred_indices.cpu().numpy())
                all_pred_probs.extend(pred_probs.cpu().numpy())

        return np.array(all_true_labels), np.array(all_pred_indices), np.array(all_pred_probs)

    def save_training_history(self, file_path: str):
        """ä¿å­˜è®­ç»ƒå†å²"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {file_path}") 